# -*- coding: utf-8 -*-
"""gectorptbr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p6PeHmbknEPlepX24EhfQRpOgHG-RmVg
"""

#%% errorify the wiki-sentences.txt file

!cd PIE/errorify/ptbr && python3 error.py ~/gectorptbrFolder/preprocessable_files/wiki-sentences.txt ~/gectorptbrFolder/files/dual_files 0.01

#%% install the pip3 dependencies

# !pip3 install -q -r gectorptbrFolder/requirements.txt

#%% generate source file for training by comparison

!python3 utils/preprocess_data.py -s files/dual_files/incorr_sentences.txt -t files/dual_files/corr_sentences.txt -o files/neural_files/inputs_labels.txt

#%% separate the inputs_labels.txt into train and test datasets

import random
import math
import pandas as pd

# open the relevant files
inputs_labels_path = 'files/neural_files/inputs_labels.txt'
train_path = 'files/neural_files/train_50.txt'
test_path = 'files/neural_files/test_50.txt'

# if the file is huge, we have to think of something else, 
# such as "linecache" instead of reading everything into memory
def shuffle_split(inpath, outpath_1, outpath_2, proportion_1=0.9, cut=0.05, except_token_regex=None, except_regex=None):
    assert proportion_1 < 1.0, 'proportion_1 must be smaller than 1.0'
    assert cut <= 1.0, 'cut cannot be larger than 1.0'
    
    with open(inpath, 'r') as f:
        lines = f.readlines()
    # append a newline in case the last line didn't end with one
    # so that when we shuffle, we do not end up with two lines
    # without the break character54
    lines[-1] = lines[-1].rstrip('\n') + '\n'
    
    random.shuffle(lines)
    lines = lines[:math.floor(len(lines)*cut)]
    
    cut_ratio = 0
    if except_token_regex is not None or except_regex is not None:
        lines_df = pd.DataFrame(lines, columns=['line'])
        if except_regex is not None:
            regularex = except_regex
        else:
            regularex = '[\s\,\"\']' + except_token_regex + '(?:SEPL|\,\.)'
        mask_with_exc = lines_df['line'].str.contains(regularex)
        lines_wo_exc = lines_df[~mask_with_exc]
        lines = lines_wo_exc['line'].values
        cut_ratio = sum(mask_with_exc)/sum(~mask_with_exc)
    
    cutoff = math.floor(proportion_1*len(lines))
    with open(outpath_1, 'w') as f:
        f.writelines(lines[:cutoff])
    with open(outpath_2, 'w') as f:
        f.writelines(lines[cutoff:])
    
    return cut_ratio
    
shuffle_split(inputs_labels_path, train_path, test_path, proportion_1=0.9, cut=0.5, except_regex='\$APPEND')

#%% run the script for fine-tuning of BERTimbau

"""
Here we have to input:
*   The training dataset; 
*   The test dataset;
*   The model directory, here as MODEL_DIR;
*   The transformer BERT model which will be fine-tuned;
*   Inform whether we want to lowercase the tokens or not (0 => no, 1 => yes). Note: the BERTimbau model was trained on cased sentences, so it would be a waste of information if we were to train on uncased (i.e. lowercased tokens only) sentences.
*   The number of epochs.
"""

### train the model
!python3 train.py --train_set files/neural_files/train.txt --dev_set files/neural_files/test.txt --model_dir MODEL_DIR --transformer_model bertimbaubase --lowercase_tokens 0 --n_epoch 5


#%% make inference on new data

"""
Here we have to input:
*   The trained model (one of the models saved in MODEL_DIR);
*   The vocabulary folder in which there is the information of what the model encountered in the training dataset, e.g. the labels \$KEEP or \$TRANSFORM_VERB_VB_VBD;
*   The file to make the inference on, e.g. a file containing sentence such as 'Eles era feios.' so that we want the neural network to output 'Eles eram feios.';
*   The file where the inference is outputted to;
*   The transformer model which we fine-tuned in the training.
"""
eval_path = '/home/ernany/gectorptbrFolder/eval_after_train.txt'
with open(eval_path, 'w') as f:
    f.writelines('Eles, fazem isso.')

!python3 predict.py --additional_confidence 0. --min_error_probability 0. --model_path MODEL_DIR/best.th --vocab_path MODEL_DIR/vocabulary/ --input_file eval_after_train.txt --output_file OUTPUT_FILE.txt --transformer_model bertimbaubase

#0.6 is a good additional_confidence

#%% check the output

print('\nINPUT')
with open(eval_path, 'r') as f:
    for line in f:
        print(line)
print('\nOUTPUT')
output_path = '/home/ernanyschmitz/gectorptbrFolder/OUTPUT_FILE.txt'
with open(output_path, 'r') as f:
    for line in f:
        print(line)








