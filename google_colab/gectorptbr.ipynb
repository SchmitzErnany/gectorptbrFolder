{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gectorptbr.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WXGrJP0kqudE",
        "PyHPSY58b4I7",
        "Ofd2TWLWmaX0",
        "NfyzbfEAmosK",
        "2IWPdrrRmvyV",
        "ORHOMxh4pfiW",
        "xzo0RM99pwms",
        "PAZ-5r77dKqJ",
        "krjxz4M52Nv0",
        "o1PFWKn1dTfN",
        "dtsabyGqeRu9",
        "O-MRbwPS4Vh2"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXGrJP0kqudE"
      },
      "source": [
        "# **Data download**\n",
        "\n",
        "> Download the data from its storage place into the Google Colab working directory\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyHPSY58b4I7"
      },
      "source": [
        "## *Data stored in a GCS Bucket*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofd2TWLWmaX0"
      },
      "source": [
        "### Log into the google account and initialize gcloud SDK\n",
        "\n",
        "> Make sure to have a google account for authentication\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUdjqdFc172X"
      },
      "source": [
        "import os, subprocess\n",
        "\n",
        "### authenticate into google account (must have a google account)\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "### install GCloud SDK\n",
        "!curl https://sdk.cloud.google.com | bash\n",
        "### initialize SDK\n",
        "!gcloud init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfyzbfEAmosK"
      },
      "source": [
        "### Set the name of the Google Cloud Storage Bucket\n",
        "\n",
        "> This directory must have already been created through the Google Cloud Platform\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyCDndfQmj_0"
      },
      "source": [
        "### define the Google Cloud Storage Bucket\n",
        "BUCKET = 'gectorptbrstorage'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IWPdrrRmvyV"
      },
      "source": [
        "### Download the file to be preprocessed\n",
        "\n",
        "> This file will be copied from the GCS Bucket into the Google Colab working directory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-f1IOL2mvDI"
      },
      "source": [
        "### download sentences file\n",
        "os.system('gsutil cp gs://' + BUCKET + '/files/wiki-sentences.txt .');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORHOMxh4pfiW"
      },
      "source": [
        "## *Data stored in a Google Drive*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzo0RM99pwms"
      },
      "source": [
        "### Download the file to be preprocessed\n",
        "\n",
        "> This file will be copied from a Google Drive into the Google Colab working directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLl6F7Hap3bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "abf96626-61b6-4ba3-809a-0066448067ce"
      },
      "source": [
        "### download sentences file\n",
        "os.system('cp /content/drive/MyDrive/gectorptbrFolder/preprocessable_files/wiki-sentences.txt .');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-800651d8f106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### download sentences file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cp /content/drive/MyDrive/gectorptbrFolder/preprocessable_files/wiki-sentences.txt .'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAZ-5r77dKqJ"
      },
      "source": [
        "# **Errorify the data**\n",
        "\n",
        "> Synthetically produce errors into the dataset and save two files in the folder dual_files/, one is for correct sentences and the other is for the errorified sentences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1z2vOxPtn3_"
      },
      "source": [
        "### errorify the wiki-sentences.txt file\n",
        "!cd /content/drive/MyDrive/gectorptbrFolder/PIE/errorify/ptbr && python3 error.py /content/wiki-sentences.txt /content/drive/MyDrive/gectorptbrFolder/files/dual_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krjxz4M52Nv0"
      },
      "source": [
        "# **Install pip3 dependencies**\n",
        "\n",
        "> The following two sections depend on these packages\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUu2ku9o2M0X"
      },
      "source": [
        "### install the package requirements\n",
        "!pip3 install -q -r /content/drive/MyDrive/gectorptbrFolder/requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1PFWKn1dTfN"
      },
      "source": [
        "# **Compare the dual files**\n",
        "> In this step, we compare the correct sentences against the errorified sentences in order to create a dataset which has inputs alongside the respective labels, for example: **Eu vou a{replace_à} praia...** . In this example, **a** is the input and its label is **replace_à**. This process creates the training file for Deep Learning.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4I5Vdyl3goE",
        "outputId": "930a2260-e780-4e74-bf39-feb570050ede"
      },
      "source": [
        "### generate source file for training\n",
        "!cd /content/drive/MyDrive/gectorptbrFolder && python3 utils/preprocess_data.py -s /files/dual_files/corr_sentences.txt -t /files/dual_files/incorr_sentences.txt -o /files/neural_files/inputs_labels.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of raw dataset is 0\n",
            "\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n",
            "Overall extracted 0. Original TP 0. Original TN 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtsabyGqeRu9"
      },
      "source": [
        "# **Fine-tune the BERTimbau model**\n",
        "> Here we fine-tune the BERTimbau model on our wikipedia dataset \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-MRbwPS4Vh2"
      },
      "source": [
        "## Separate the inputs_labels.txt file into training and testing datasets\n",
        "\n",
        "> We create a function ourselves since we have inputs and labels together in one file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8GyAx7O5CC-"
      },
      "source": [
        "from random\n",
        "from math\n",
        "\n",
        "# open the relevant files\n",
        "inputs_labels_path = '/content/drive/MyDrive/gectorptbrFolder/files/neural_files/inputs_labels.txt'\n",
        "train_path = '/content/drive/MyDrive/gectorptbrFolder/files/neural_files/train.txt'\n",
        "test_path = '/content/drive/MyDrive/gectorptbrFolder/files/neural_files/test.txt'\n",
        "\n",
        "# if the file is huge, we have to think of something else, \n",
        "# such as \"linecache\" instead of reading everything into memory\n",
        "def shuffle_split(inpath, outpath_1, outpath_2, proportion_1=0.8):\n",
        "    assert proportion_1 < 1.0, 'proportion_1 must be smaller than 1.0'\n",
        "\n",
        "    with open(inpath, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    # append a newline in case the last line didn't end with one\n",
        "    # so that when we shuffle, we do not end up with two lines\n",
        "    # without the break character\n",
        "    lines[-1] = lines[-1].rstrip('\\n') + '\\n'\n",
        "\n",
        "    random.shuffle(lines)\n",
        "\n",
        "    cutoff = math.floor((1 - proportion_1)*len(lines))\n",
        "    with open(outpath_1, 'w') as f:\n",
        "        f.writelines(lines[:cutoff])\n",
        "    with open(outpath_2, 'w') as f:\n",
        "        f.writelines(lines[cutoff:])\n",
        "\n",
        "shuffle_split(inputs_labels_path, train_path, test_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBFcK3gV44Gy"
      },
      "source": [
        "## Run the script for fine-tuning\n",
        "\n",
        "Here we have to input:\n",
        "*   The training dataset; \n",
        "*   The test dataset;\n",
        "*   The model directory, here as MODEL_DIR;\n",
        "*   The transformer BERT model which will be fine-tuned;\n",
        "*   Inform whether we want to lowercase the tokens or not (0 => no, 1 => yes). Note: the BERTimbau model was trained on cased sentences, so it would be a waste of information if we were to train on uncased (i.e. lowercased tokens only) sentences.\n",
        "*   The number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKDezqqNFR7X"
      },
      "source": [
        "### train the model\n",
        "!cd /content/drive/MyDrive/gectorptbrFolder && python3 train.py --train_set files/neural_files/train.txt --dev_set files/neural_files/test.txt --model_dir MODEL_DIR --transformer_model bertimbau --lowercase_tokens 0 --n_epoch 5\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPRHg8ErEfez"
      },
      "source": [
        "## Make inference on a file\n",
        "\n",
        "Here we have to input:\n",
        "*   The trained model (one of the models saved in MODEL_DIR); \n",
        "*   The vocabulary folder in which there is the information of what the model encountered in the training dataset, e.g. the labels \\$KEEP or \\$TRANSFORM_VERB_VB_VBD;\n",
        "*   The file to make the inference on, e.g. a file containing sentence such as 'Eles era feios.' so that we want the neural network to output 'Eles eram feios.';\n",
        "*   The file where the inference is outputted to;\n",
        "*   The transformer model which we fine-tuned in the training.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzc3uGmeErq3",
        "outputId": "da049a37-b94e-4bf5-c207-94c1ff1b35fa"
      },
      "source": [
        "!cd /content/drive/MyDrive/gectorptbrFolder && python3 predict.py --model_path MODEL_DIR/model_state_epoch_4.th --vocab_path MODEL_DIR/vocabulary/ --input_file eval_after_train.txt --output_file OUTPUT_FILE.txt --transformer_model bertimbau\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"predict.py\", line 4, in <module>\n",
            "    from gector.gec_model import GecBERTModel\n",
            "  File \"/content/drive/MyDrive/gectorptbrFolder/gector/gec_model.py\", line 8, in <module>\n",
            "    from allennlp.data.dataset import Batch\n",
            "ModuleNotFoundError: No module named 'allennlp'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtCR_IuiLpQ9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}